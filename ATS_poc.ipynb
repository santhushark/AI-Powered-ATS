{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96c29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff99521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_list_in_folder(folder_path):\n",
    "    file_list = [f\"{folder_path}/{f}\" for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a5463a-d4aa-4f0b-8aa1-14db721a5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_contents = []\n",
    "resume_files = get_files_list_in_folder(\"Resumes\")\n",
    "for file in resume_files:\n",
    "    with pdfplumber.open(file) as pdf:\n",
    "        resume_contents.append(pdf.pages[0].extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218d9cc5-7829-4400-8035-e3b0ade22f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lakshmi Himaja Amrutham\n",
      "IL,USA • (312) 7742876 • alakshmihimaja@hawk.iit.edu • linkedin.com/in/laksh\n"
     ]
    }
   ],
   "source": [
    "full_text = \"\".join(resume_contents[0])\n",
    "print(full_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9d1ac9-c00d-4a6e-8eca-819aeacd4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET secret keys\n",
    "load_dotenv()\n",
    "antropic_claude_api_key = os.getenv(\"ANTHROPIC_CLAUDE_API_KEY\")\n",
    "weaviate_cluster_url = os.getenv(\"WEAVIATE_CLUSTER_URL\")\n",
    "weaviate_api_key = os.getenv(\"WEAVIATE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPEN_AI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda78010-2f95-4d4f-8ca3-6b9460fc5801",
   "metadata": {},
   "source": [
    "## Generating summaries for each resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35756a7a-2970-4f3a-99e6-725a2709a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain, SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd16f595-f880-4cfb-950f-29b62be31b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Anthropic Claude model\n",
    "claude = ChatAnthropic(temperature=0.9, \n",
    "                       api_key=antropic_claude_api_key, \n",
    "                       model_name=\"claude-3-5-sonnet-20241022\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024f16f-d498-423e-9e3b-a7d93d3d78fa",
   "metadata": {},
   "source": [
    "#### Building sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec503c93-d224-4ee0-9cef-a9df6902bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_json_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Structure the following resume content in JSON format\"\n",
    "    \"\\n\\n{resume_content}\")\n",
    "# chain 1: input= resume_content and output= resume content in structured json format\n",
    "chain_one = LLMChain(llm=claude, prompt=gen_json_prompt, output_key=\"resume_json\")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate a 300 words summary for the resume content in JSON format below.\" \n",
    "    \"Highlight all the skills, education, certifications if any, responsibilities and publications.\" \n",
    "    \"\\n\\n{resume_json}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=claude, prompt=summary_prompt, output_key=\"resume_summary\")\n",
    "\n",
    "# constructing the sequential chain\n",
    "resume_summary_chain = SequentialChain(chains=[chain_one, chain_two],\n",
    "                                       input_variables=[\"resume_content\"],\n",
    "                                       output_variables = [\"resume_summary\"],\n",
    "                                       verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b1da6e7-6323-4f50-92a3-2ef202723439",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_summaries = []\n",
    "for r_content in resume_contents:\n",
    "    res = resume_summary_chain(r_content)\n",
    "    resume_summaries.append(res[\"resume_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1434ba4e-c880-4231-a157-ec2686771751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the resume summaries for future use\n",
    "for r_summary in resume_summaries:\n",
    "    r_summary = r_summary[42:]\n",
    "    name = \"\".join(r_summary.split()[:2])\n",
    "    with open(f\"summary/{name}_resume_summary.txt\", \"w\") as file:\n",
    "        file.write(r_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec450708-4ba0-4365-896d-b23b30a94ce8",
   "metadata": {},
   "source": [
    "## Saving the Resume summaries in Vector Data store Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dbd8bb1-838b-40d6-b3b8-1cfc129e8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.util import generate_uuid5\n",
    "from weaviate.classes.config import Configure, Property, DataType, Tokenization\n",
    "from weaviate.classes.query import MetadataQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651fecf9-8bb8-401a-b92f-e2254a1aee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santosh/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "/home/santosh/anaconda3/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m client\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ddba2a-e3ba-4822-88d0-4a9017bdce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the weaviate client\n",
    "def get_weaviate_client():\n",
    "    return weaviate.connect_to_weaviate_cloud(\n",
    "        cluster_url= weaviate_cluster_url, # Replace with your Weaviate Cloud URL\n",
    "        auth_credentials= Auth.api_key(weaviate_api_key), # Replace with your Weaviate Cloud key\n",
    "        headers={\"X-OpenAI-Api-Key\": openai_api_key}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdde65d8-9689-4a9a-809b-188202389c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create or get summary collection\n",
    "def create_or_get_summary_collection(coll_name, properties, client):\n",
    "    if client.collections.exists(coll_name):\n",
    "        return client.collections.get(coll_name)\n",
    "    else:\n",
    "        client.collections.delete(coll_name)\n",
    "        client.collections.create( \n",
    "            name = coll_name,\n",
    "            vectorizer_config=[\n",
    "                # Use the \"text2vec-openai\" vectorizer\n",
    "                Configure.NamedVectors.text2vec_openai(  \n",
    "                name=properties[1], source_properties=[properties[1]]         # Set the source property(ies)\n",
    "                )],\n",
    "            properties=[ \n",
    "                Property(name=properties[0], \n",
    "                         data_type=DataType.TEXT),\n",
    "                Property(name=properties[1], \n",
    "                         data_type=DataType.TEXT, \n",
    "                         vectorize_property_name=True,\n",
    "                         tokenization=Tokenization.LOWERCASE)\n",
    "            ],\n",
    "            # vectorizer_config=Configure.Vectorizer.text2vec_openai(),   # Configure the OpenAI embedding integration\n",
    "            generative_config=Configure.Generative.openai()             # Configure the OpenAI generative AI integration\n",
    "        )\n",
    "        return client.collections.get(coll_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed1976f-abe1-4e24-a341-a866f81e984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Get client and check if its ready\n",
    "client = get_weaviate_client()\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1891964f-676e-4cf6-b3fb-2c0383228e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes collections / do not use unless necessary\n",
    "client.collections.delete(\"resume_summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df2a299c-d587-4a87-bc92-14f7e2d9f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING the collection \"resume_summary\"\n",
    "properties = [\"candidate_summary_file\", \"summary\"]\n",
    "collection_name = \"resume_summaries\"\n",
    "resume_summaries = create_or_get_summary_collection(collection_name, properties, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fd4713f-4ae1-409c-86be-8ac15a412f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to persist data objects to the collection\n",
    "def persist_data_to_collection(resume_summaries, resume_summary_files):\n",
    "    for summary_file in resume_summary_files:\n",
    "        with open(summary_file, \"r\") as summary:\n",
    "            data_object = {\n",
    "                \"candidate_summary_file\": summary_file[16:-4],\n",
    "                \"summary\": summary.read(),\n",
    "            }\n",
    "            resume_summaries.data.insert(\n",
    "                properties=data_object,\n",
    "                uuid=generate_uuid5(data_object),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d70cc465-1d9c-4df0-94ff-2707bdaa0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting resume summary data objects to the collection\n",
    "resume_summary_files = get_files_list_in_folder(\"Resumes/summary\")\n",
    "persist_data_to_collection(resume_summaries, resume_summary_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ae989-5c24-4899-8eb1-428ec0498d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the objects inside a collection\n",
    "for item in resume_summaries.iterator():\n",
    "    print(item.uuid, item.properties)\n",
    "    break # remove break to print all the data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aba754e9-2372-4228-a88c-40d7ba2ed35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"\"\"Experience with Large Language Model (LLM) APIs. \n",
    "Experience working with Machine Learning and Deep Learning Libraries. \n",
    "Proven track record of delivering complex, scalable, and high-performance software systems. \n",
    "Knowledge on SKlearn, Python, Neural Networks, Transformers, Keras, Tensorflow and Pytorch.   \n",
    "\"\"\"\n",
    "\n",
    "response = resume_summaries.query.near_text(\n",
    "    query=job_description,\n",
    "    limit=4,\n",
    "    target_vector=\"summary\",  # Specify the target vector for named vector collections\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0c88051-adf6-4f85-a7a2-ac143903ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karthik Kaiplody is a Machine Learning Engineer pursuing a Master's in Data Science at Illinois Institute of Technology. He holds a Post Graduate Diploma in ML and AI from IIIT Bangalore and a Bachelor's in Electronics and Communication.\n",
      "\n",
      "Skills Highlights:\n",
      "- Technical: AWS, Python, Scikit-Learn, TensorFlow, Keras, PyTorch, NLTK, spaCy, Flask, Git, SQL\n",
      "- Core Competencies: MLOps, Data Pipeline Development, NLP, Deep Learning, Computer Vision, CI/CD\n",
      "- Functional: Teamwork, Client Management, Interpersonal Communication\n",
      "\n",
      "Professional Experience:\n",
      "Currently working as Data Science Engineer R&D Intern at CCC Intelligent Solutions, focusing on CI/CD pipeline development for AI products and AWS migration. Previously served as Research Assistant at Illinois Institute of Technology's SCS Laboratory, developing machine learning pipelines and nutritional data tools.\n",
      "\n",
      "At Acuity Knowledge Partners, as Machine Learning Engineer III, he:\n",
      "- Implemented transformer-based models using MLOps practices\n",
      "- Developed serverless architecture using AWS Lambda\n",
      "- Automated data engineering pipeline using AWS Step-functions\n",
      "\n",
      "During his tenure at Mindtree Ltd as Senior Engineer, he worked on ETL operations and BERT/ELMO-based embeddings for NLP classifiers.\n",
      "\n",
      "Education:\n",
      "- Master's in Data Science (Illinois Institute of Technology, Expected 2024)\n",
      "- PG Diploma in ML and AI (IIIT Bangalore, 2021)\n",
      "- BE in Electronics and Communication (VTU, 2016)\n",
      "\n",
      "Notable Achievements:\n",
      "- Reduced resource costs by 30% through microservice architecture implementation\n",
      "- Minimized model turnaround time from a week to hours using MLOps pipeline\n",
      "- Experience across multiple domains including Networking, FinTech, and Insurance\n",
      "\n",
      "His expertise combines strong technical skills in machine learning with practical experience in implementing enterprise-level solutions using cloud technologies and modern development practices.\n",
      "0.40486305952072144\n",
      "\n",
      "\n",
      "\n",
      "Shreesha N Murthy is a Machine Learning Engineer with extensive experience in AI and software development. He holds a Master's degree in Data Science from Worcester Polytechnic Institute (GPA: 4.0/4.0) and a Bachelor's in Computer Science from VT University.\n",
      "\n",
      "Skills Highlights:\n",
      "- Programming Languages: Python, Java, Scala, JavaScript, TypeScript, R\n",
      "- ML/AI Frameworks: TensorFlow, PyTorch, Hadoop, Spark, Kafka\n",
      "- Machine Learning: Neural Networks (CNN, LSTM, Transformers), SVM, Random Forests, XGBoost\n",
      "- MLOps: Airflow, TensorflowX, Docker, AWS, GCP\n",
      "- Databases: SQL, MongoDB, Cassandra\n",
      "\n",
      "Professional Experience:\n",
      "Currently working at Cushion.ai (San Francisco) as a Machine Learning Engineer, focusing on credit-risk assessment and consumer bill detection. Previously served at RazorThink Technologies (India) as an AI Engineer, where he achieved significant results including:\n",
      "- Character Recognition solution with 92% accuracy\n",
      "- Resume parsing and ranking system\n",
      "- Doc2Vec model with 94% sentence accuracy\n",
      "- CNN prediction explanation system\n",
      "\n",
      "Research and Publications:\n",
      "- Conducted research on Differential Learning using Neural Network Pruning, achieving 85% accuracy on Fashion-MNIST\n",
      "- Published two papers:\n",
      "  1. \"DeepSEAS: Ailment Sensing using Coupled LSTM Autoencoders\" (IEEE BigData 2020)\n",
      "  2. \"Deep Anomaly Detection methods to passively detect COVID-19 from Audio\" (ICDH 2020)\n",
      "\n",
      "Education:\n",
      "His academic background includes relevant coursework in Statistics, Big Data Analytics, Machine Learning, Deep Learning, and Business Intelligence.\n",
      "\n",
      "This combination of technical skills, research experience, and practical implementation makes him a well-rounded professional in the machine learning and AI domain, with particular expertise in deep learning and big data technologies.\n",
      "0.4432002305984497\n",
      "\n",
      "\n",
      "\n",
      "Prathyush S Parvatharaju is an AI Engineer currently serving as a Journeyman Fellow at U.S. Army CCDC Army Research Laboratory. With over 5 years of experience in AI pipeline development, he specializes in black-box model explanations and active explainers for CNNs.\n",
      "\n",
      "Education:\n",
      "- M.S. in Data Science from Worcester Polytechnic Institute (GPA: 4.0)\n",
      "- B.E. in Electronics and Communication from VT University (GPA: 3.7)\n",
      "\n",
      "Technical Skills:\n",
      "- Programming Languages: Python, Javascript, Java, C++, C, R, HTML\n",
      "- Machine Learning: SVM, RandomForests, Neural Networks, Reinforcement Learning\n",
      "- MLOps: Docker, Kubernetes, Airflow, TensorflowX\n",
      "- Frameworks: PyTorch, TensorFlow, Keras, Flask, OpenCV\n",
      "- Big Data: Spark, CUDA, Hadoop, Kafka\n",
      "- Databases: MySQL, MongoDB, Neo4J, Redis\n",
      "\n",
      "Professional Experience:\n",
      "At U.S. Army Research Laboratory (2020-Present):\n",
      "- Led and trained a 20-member team in ML research\n",
      "- Architected Context-Aware Shared Agile Platform\n",
      "\n",
      "At Razorthink Technologies (2015-2019):\n",
      "- Developed distributed modeling library\n",
      "- Achieved 97% accuracy with scaled CRNNs\n",
      "- Led LSTM model development for customer churn prediction\n",
      "- Designed multi-stage CNN for time-series data\n",
      "\n",
      "Publications:\n",
      "1. \"Learning Saliency Maps to Explain Deep Time Series Classifiers\" (CIKM '21)\n",
      "2. \"Differential Learning using Neural Network Pruning\" (CORR'21)\n",
      "\n",
      "His expertise spans across various domains of AI/ML, including deep learning, distributed systems, and MLOps. His work demonstrates strong leadership capabilities combined with technical excellence in developing and implementing cutting-edge AI solutions. His perfect academic record and research publications showcase his commitment to continuous learning and contribution to the field.\n",
      "0.46949148178100586\n",
      "\n",
      "\n",
      "\n",
      "Lakshmi Himaja Amrutham is a computer science professional with a Master's degree from Illinois Institute of Technology (GPA: 3.33/4.0) and a Bachelor's degree from Anna University (GPA: 3.7/4.0).\n",
      "\n",
      "Education & Relevant Coursework:\n",
      "- Master's coursework includes Data Mining, Machine Learning, Cloud Computing, Big Data Technologies, and Mobile Application Development\n",
      "- Bachelor's coursework includes Data Structures and Algorithms, Object-Oriented Analysis and Design, and Database Management System\n",
      "\n",
      "Technical Skills:\n",
      "- Programming Languages: C, C++, C#, Java, Python\n",
      "- Databases: Oracle MySQL, SQL, AWS DynamoDB\n",
      "- Web Development: HTML, CSS, JavaScript, .Net, XML, XSLT Schema\n",
      "- Tools: Jenkins, Git, Visual Studio Code, PyCharm, Eclipse, BMC Remedy, TIBCO BW, Jira, Splunk\n",
      "- Cloud Technologies: AWS S3, AWS EMR, Chameleon Cloud\n",
      "- Libraries/Technologies: Linux, Pandas, Numpy, Seaborn, BeautifulSoup, Hadoop\n",
      "\n",
      "Professional Experience:\n",
      "Currently working as a Pricing Data Science Intern at Camping World, Chicago, where she:\n",
      "- Analyzes large datasets (up to 10 million rows)\n",
      "- Performs Exploratory Data Analysis\n",
      "- Develops Machine Learning models for price prediction\n",
      "\n",
      "Previously worked as a Senior Systems Engineer at Infosys (2017-2020), where she:\n",
      "- Implemented JavaScript in OT Integration Center\n",
      "- Managed deployment scripts\n",
      "- Designed BW processes for EMS/JMS servers\n",
      "\n",
      "Project Experience includes working with NoSQL databases and web development projects like 'Course Demy' and 'Food Orders'.\n",
      "\n",
      "The combination of her educational background, technical skills, and professional experience demonstrates her expertise in data science, software development, and system engineering, with a strong focus on machine learning and data analysis.\n",
      "0.49529075622558594\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the respose\n",
    "for o in response.objects:\n",
    "    print(o.properties[\"summary\"])\n",
    "    print(o.metadata.distance)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6e809-bb3d-41bb-a132-9abe02aae8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
